---
title: "When Should One Stop Testing Software?"
author: "Michael Höhle <[http://www.math.su.se/~hoehle](http://www.math.su.se/~hoehle)>"
date: "6 May 2016"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,fig.width=6,fig.height=3)
library("ggplot2")
library("dplyr")
```


# Abstract

This is a small note rediscovering a gem published by S. R. Dalal and C. L. Mallows in 1988 about treating the test of software in a statistical context.
They answer the question on how long to continue testing your software before
shipping it by finding an optimal stopping rule in order to minimize expected loss.
We sketch the main result of their article and apply their rule to an example
using R code.

# Introduction

Imagine that a team of developers of a new R package needs to structure a test
plan before the release of the package to CRAN. Let $N$ be the (unknown) number
of bugs in the package. They start their testing at time zero and subsequently
find an increasing number of bugs as the test period passes by. The figure below
shows such a testing process mimicking the numbers of the example
in Dalal and Mallows (1988), which originates from the testing of a large
software system at a telecommunications research company.

```{r,echo=FALSE,fig.align='center',results='hide',fig.keep='last'}
#Simulate some data, which look like Fig. 1 of the paper. 
set.seed(123)
t <- 0:175
mu <- 0.02
n <- 1250
#Simulate the inter arrival times of the exponential distribution
#See
#http://www.math.kth.se/matstat/gru/sf2955/exponorderstats.pdf
K <- cumsum(rexp(1250,rate=(n+1-1:1250)*mu))
#Discretize the event times into daily data
K_days <- head(cumsum(table(cut(K, breaks=c(0:(max(t)+1),Inf)))),n=-1)
K_days
#Show result
testProcess <- data_frame(t=t,K=K_days)
p <- ggplot(testProcess, aes(x=t,y=K)) + geom_line() + xlab("Time (testing days)") + ylab("Cumulative Number of Bugs found")
print(p)

```

We see that the number of bugs appears to level off. The question is now *how long should we continue testing*? Dalal and Mallows
(1988) give a intriguing statistical answer to this problem.

# Methodology

In order to answer the above question the following notation and assumptions are introduced.

* The total number of bugs is assumed to be Poisson distributed $$N|\lambda \sim \text{Po}(\lambda).$$ However, practice shows that the number of bugs in different modules has more variation that given by the Poisson distribution. Hence, let $\lambda \sim \text{Ga}(\alpha,\beta)$ and thus the marginal distribution of $N$ is negative binomial. 

* The amount of time until discovery of each bug during the testing period is distributed according to the known distribution $G$ with density $g$ and that the discoveries are independent of each other. 
The simplest example is to assume that the discovery distribution is exponential, i.e. $g(t)=\mu\exp(-\mu t)$, 
where we measure time in number of person-days spent on the testing. 
Thus, $1/\mu$ is the expected time until discovery of a bug.

* Let $K(t)$ be the total number of bugs found up to time $t$. Assuming that the inter-arrival times (i.e. the waiting time between two events) of the bug-discovery process are $0=t_{(0)} < t_{(1)} < t_{(2)} < \ldots < t_{(K(t))}$. We thus have that $$K(t) = \sum_{i : t_{(i)} \leq t}  t_{(i)}.$$

Note: The paper actually showns that the Poisson-Gamma distribution assumption for $N$ is not crucical. An asymptotic argument is given that as long as the process does not terminate quickly (i.e. the number of bugs is relatively large) the results hold for more general distributions of $N$. Hence, in what follows, the parameter $\lambda$ is not needed and we proceed only with the asymptotic approach of the paper. 

### Loss function
In order to make a decision about when to stop testing based on expected loss/gain we need two further assumptions:

* Let $c$ be the net cost of fixing a bug *after* release of the software instead of *before* the release. Hence, $c$ is the price of fixing a bug after release minus the price of fixing a bug before release. The practice of software development tells us $c>0$ -- this applies even to Open Source software.

* Let $f(t)$ be a known non-negative and monotone increasing function reflecting the cost of testingplus the opportunity cost of not releasing the software up to time $t$. Note that the cost of testing does not contain the costs of fixing bugs, once they are found. A simple example for $f$ is the linear loss function, i.e. $f(t) = f \cdot t$, where $f>0$ is a known constant.

The above assumptions imply the analysis of the following loss function:

$$L(t,K(t),N) = f(t) - c K(t) + b\cdot N.$$

As time passes one obtains information about the number of bugs found through $K(t)$. At each time point the following decision has to be made: stop testing & ship the package or continue to test. Seen in a statistical context this can
be rephrased as how to pick the stopping rule s.t. the above loss function is minimized?

### Optimal Stopping Time

In the simple model with exponential discovery times with rate $\mu$ the rule found as equation (4.6) of Dalal and Mallows (1988) is to stop as soon as the number, $k$, of bugs found at time $t$ is such that:
$$
\frac{f}{c}\cdot \frac{\exp(\mu t) -1}{\mu} \geq k.
$$
The estimated number of bugs left is Poisson with mean $f/(c\mu)$.

In the above, the quantity $c/f$ measures the amount saved by finding a bug (and hence fixing it before release) measured in units of testing days. As an example: if $c/f=0.2$ then the gain in detecting a bug before release corresponds to 0.2 testing days.

# Example

Taking the testing data from the above figure, the first step consists of estimating $\mu$ from the available data. It's important to realize that the available data are a right-truncated sample, because only errors with a 
discovery time smaller than the current observation time are observed.
Furthermore, if data on the daily number of bug discoveries then the data are also interval censored. We setup up the loglikelihood function accordingly.

```{r}
#Daily number of *new* bug discoveries. The variable K_days contains the cumulated
#number of bugs discovered as daily time series.
DeltaK <- c(0,diff(K_days))

#######################################################
#Log-likelihood function to maximize, which handles the
#right truncation and interval censoring.
# Paramers:
#  theta - \log(\mu).
#  tC - the right-censoring time.
########################################################
ll <- function(theta, tC=176) {
  mu <- exp(theta)
  CDF <- function(x) pexp(x,rate=mu)/pexp(tC,rate=mu)
  sum(DeltaK * log(CDF(1:(max(t)+1)) - CDF(t)))
}
mle <- optim(log(0.01),ll, control=list(fnscale=-1),method="BFGS")
mu.hat <- exp(mle$par)
c(mu=mu, mu.hat=mu.hat)
```

Note that we in the above used all data obtained over the entire testing
period. In practice, one would instead sequentially update the $\mu$ estimate each day as the information arrives.

```{r,warning=FALSE}
#Function describing the LHS of (4.6) in the Delal and Mallows article
# fdivc - f/c
lhs <- function(fdivc) {
  fdivc*(exp(mu*t)-1)/mu
}

#Add curve
testProcess <- testProcess %>% mutate(sol5=lhs(5), sol1=lhs(1))
p + geom_line(data=testProcess, aes(x=t,y=sol5),lty=2) + ylim(c(0,1600))
#Determine the stopping time for a f/c ratio of 5.
stopTime <- testProcess %>% filter(sol5 >= K) %>%  filter(row_number()==1)
stopTime
```

The optimal stopping time in the example is to stop the testing after `r stopTime$t` testing days. An estimate of the expected number of remaining bugs at this stopping time would be `r sprintf("%.1f",5/mu.hat)`, which appears to agree quite well with the empirical data!

# Discussion

* How realistic is it that the discovery distribution of bugs is independent between the bugs? 

* For Open Source Software and in particular R packages, which nobody might ever use, is $c$ really bigger than zero? Ship and fix might be a good way to test, if a package actually addresses any kind of need?

* How to extract the daily number of bugs found from your bug tracking ticket system? 

# Literature

* Dalal, S. R., and C. L. Mallows. “[When Should One Stop Testing Software?](http://www.jstor.org/stable/2289319)”. Journal of the American Statistical Association 83.403 (1988): 872–879. 